{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence and Efficency\n",
    "You now might be aware of some of the problems with parsing several pages and how that can quicly get out of hand. While parsing, you might chose to persist the collected data, so that it can be analyzed and cleaned later. \n",
    "Parsing, retrieving, saving, and cleaning data are all separate actions, and you shouldn't try to work with data while collecting. In this notebook you'll practice further parsing techniques along with persistency, both for JSON and CSV formats as well as using SQL and a database.\n",
    "\n",
    "Start by loading one of the available HTML files into the `scrapy` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: source: not found\n",
      "Requirement already satisfied: scrapy==2.5.1 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (2.5.1)\n",
      "Requirement already satisfied: Twisted[http2]>=17.9.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (22.10.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (2.9.2)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (19.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (21.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: h2<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (4.9.3)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.7/site-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: six>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.0->scrapy==2.5.1->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.0->scrapy==2.5.1->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/conda/lib/python3.7/site-packages (from h2<4.0,>=3.0->scrapy==2.5.1->-r requirements.txt (line 1)) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /opt/conda/lib/python3.7/site-packages (from h2<4.0,>=3.0->scrapy==2.5.1->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.7/site-packages (from itemloaders>=1.0.1->scrapy==2.5.1->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from parsel>=1.5.0->scrapy==2.5.1->-r requirements.txt (line 1)) (20.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from parsel>=1.5.0->scrapy==2.5.1->-r requirements.txt (line 1)) (3.7.4.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy==2.5.1->-r requirements.txt (line 1)) (19.3.0)\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy==2.5.1->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy==2.5.1->-r requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->scrapy==2.5.1->-r requirements.txt (line 1)) (15.1.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->scrapy==2.5.1->-r requirements.txt (line 1)) (22.10.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->scrapy==2.5.1->-r requirements.txt (line 1)) (22.10.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->scrapy==2.5.1->-r requirements.txt (line 1)) (21.0.0)\n",
      "Requirement already satisfied: priority<2.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->scrapy==2.5.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy==2.5.1->-r requirements.txt (line 1)) (46.1.3.post20200325)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy==2.5.1->-r requirements.txt (line 1)) (2.20)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/conda/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy==2.5.1->-r requirements.txt (line 1)) (2.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->parsel>=1.5.0->scrapy==2.5.1->-r requirements.txt (line 1)) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "# If you haven't already created an activated a virtual environment for this notebook, run this cell\n",
    "!python3 -m venv venv\n",
    "!source venv/bin/activate\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\t  persistence.ipynb  requirements.txt\t\t     venv\r\n",
      "parse.py  README.md\t     scraping-with-htmlparser.ipynb  wikipedia-demo\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapy==2.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ParamSpec' from 'typing_extensions' (/opt/conda/lib/python3.7/site-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5411f769b3d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcurrent_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html/1992_World_Junior_Championships_in_Athletics_â€“_Men's_high_jump\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scrapy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Declare top-level shortcuts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspiders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFormRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scrapy/spiders/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrackref\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobject_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murl_is_from_spider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scrapy/http/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scrapy/http/headers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mw3lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheaders_dict_to_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCaselessDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scrapy/utils/python.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScrapyDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scrapy/utils/decorators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScrapyDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/twisted/internet/defer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mincremental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParamSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProtocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterfaces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIDelayedCall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIReactorTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ParamSpec' from 'typing_extensions' (/opt/conda/lib/python3.7/site-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import os\n",
    "current_dir = os.path.abspath('')\n",
    "url = os.path.join(current_dir, \"html/1992_World_Junior_Championships_in_Athletics_â€“_Men's_high_jump\")\n",
    "with open(url) as _f:\n",
    "    url_data = _f.read()\n",
    "\n",
    "response = scrapy.http.TextResponse(url, body=url_data, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Steve Smith\n",
      "Silver Tim Forsyth\n",
      "Bronze Takahiro Kimino\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the interesting data is available \n",
    "table = response.xpath('//table')[1].xpath('tbody')\n",
    "for tr in table.xpath('tr'):\n",
    "    print(tr.xpath('td/b/text()').extract()[0],\n",
    "          tr.xpath('td/a/text()').extract()[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interaction with `scrapy` in a Jupyter Notebook is useful because you don't need to run the special shell and you also don't need to run the whole spider. Once you learn what you need to do here, you can adapat the spider to persist data.\n",
    "First, start by persisting data as JSON. To do this, you will need to keep the information in a Python data structure like a dictionary, and then load it as a JSON object, and finally, save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gold': 'Steve Smith', 'Silver': 'Tim Forsyth', 'Bronze': 'Takahiro Kimino'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapped_data = {}\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    scrapped_data[medal] = athlete\n",
    "\n",
    "scrapped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# You can convert Python into JSON first, but there is no need if you use `json.dump()`\n",
    "# as shown next\n",
    "json_data = json.dumps(scrapped_data)\n",
    "\n",
    "# Persist it in a file:\n",
    "with open(\"1992_results.json\", \"w\") as _f:\n",
    "    # use dump() with the Python dictionary directly. \n",
    "    # the conversion is done on the fly\n",
    "    json.dump(scrapped_data, _f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can persist the scrapped data as JSON, you can also use CSV. This is specially useful if you want to to some data science operations. Although you can use an advanced library like Pandas for this, you can use the standard library CSV module from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the data first\n",
    "\n",
    "column_names = [\"Medal\", \"Athlete\"]\n",
    "rows = []\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    rows.append([medal, athlete])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now persist it to disk\n",
    "import csv\n",
    "\n",
    "with open(\"1992_results.csv\", \"w\") as _f:\n",
    "    writer = csv.writer(_f)\n",
    "\n",
    "    # write the column names\n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    # now write the rows\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can persist data to a database. Unlike the JSON and CSV approach, using a database is much more memory efficient. This is the principal reason why you want to use a database instead of a file on disk. Imagine capturing 10GB of data. This could potentially mean that you need 10GB of available memory to hold onto that data before saving it to disk.\n",
    "By using a database, you can save the data as the data is gathered. \n",
    "\n",
    "For the next cells, use a SQLite database to persist the data. Create the file-based database and the table needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "db_table = 'CREATE TABLE results (id integer primary key, medal TEXT, athlete TEXT)'\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(db_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is time to persist the data. Open the connection again\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "cursor = connection.cursor()\n",
    "query = 'INSERT INTO results(medal, athlete) VALUES(?, ?)'\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    cursor.execute(query, (medal, athlete)) \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now persisted in a file-based database that you can query. Verify that all works by creating a new connection and querying the database.\n",
    "\n",
    "Update the _wikipedia_ project and spider to use some of these techniques to persist data. Next, try parsing all the files in the _html_ directory instead of just one and persist all results. Do you think you can parse other information as well? \n",
    "\n",
    "Try parsing the height and the results from all the other athletes, not just the top three places."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0303190f1c8c691fa9994d3c799a212d90e80d675371cad4184da4200e89748e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
